{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqx1TKRxIm2EVMs/A+vkKF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "88lL5iE2gPqS"
      },
      "outputs": [],
      "source": [
        "#  What is Entropy in Decision Trees?\n",
        "\n",
        "# Entropy measures the impurity (uncertainty) in a dataset.\n",
        "\n",
        "# A decision tree uses entropy to decide where to split the data.\n",
        "\n",
        "# Goal: split the data in such a way that each branch\n",
        "# becomes more pure (less entropy).\n",
        "\n",
        "#   purity vs impurity\n",
        "\n",
        "# üîπ Purity\n",
        "\n",
        "# A node (or dataset) is called pure if all the samples\n",
        "# inside it belong to the same class.\n",
        "\n",
        "# Example:\n",
        "\n",
        "# If a node contains 100 animals and all are Dogs, then the node is pure.\n",
        "\n",
        "# In this case, there is no uncertainty.\n",
        "\n",
        "# Entropy = 0, Gini = 0.\n",
        "\n",
        "# Pure means perfectly classified (no mix of classes).\n",
        "\n",
        "#  Impurity\n",
        "\n",
        "# A node is impure if it contains a mix of different classes.\n",
        "\n",
        "# Example:\n",
        "\n",
        "# If a node has 50 Dogs and 50 Cats ‚Üí maximum impurity (most uncertain).\n",
        "\n",
        "# If a node has 80 Dogs and 20 Cats ‚Üí still impure,\n",
        "#  but less than the 50‚Äì50 case.\n",
        "\n",
        "#  Impure means uncertain / mixed classes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How Decision Trees Use This\n",
        "\n",
        "\n",
        "# Decision trees split data so that impurity decreases after each split.\n",
        "\n",
        "# Measures of impurity:\n",
        "\n",
        "# Entropy ‚Üí 0 (pure) to 1 (most impure in binary case).\n",
        "\n",
        "# Gini Index ‚Üí 0 (pure) to 0.5 (most impure in binary case)."
      ],
      "metadata": {
        "id": "R3Nbjn6Pgk-k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gini\n",
        "\n",
        "# üîπ Properties\n",
        "\n",
        "# Pure node ‚Üí only one class present ‚Üí Gini = 0\n",
        "\n",
        "# Most impure node (equal distribution of classes) ‚Üí Gini is maximum"
      ],
      "metadata": {
        "id": "6u1_b2YUixIB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# observation\n",
        "\n",
        "#  1. more than the uncertainity more is entropy\n",
        "\n",
        "# define\n",
        "\n",
        "#  Entropy and Uncertainty\n",
        "\n",
        "# Entropy is a measure of uncertainty (or impurity).\n",
        "\n",
        "# If the outcome is certain ‚Üí entropy is low (0).\n",
        "\n",
        "# If the outcome is uncertain / random ‚Üí entropy is high.\n",
        "\n",
        "\n",
        "# example\n",
        "\n",
        "#  Binary Classification Example\n",
        "\n",
        "# 100% Dog (no Cat)\n",
        "\n",
        "# No uncertainty, you already know the class.\n",
        "\n",
        "# Entropy = 0 (pure).\n",
        "\n",
        "# 50% Dog, 50% Cat\n",
        "\n",
        "# Maximum uncertainty ‚Üí you can‚Äôt predict better than random guess.\n",
        "\n",
        "# Entropy = 1 (maximum).\n",
        "\n",
        "# 80% Dog, 20% Cat\n",
        "\n",
        "# Some uncertainty (but less than 50‚Äì50).\n",
        "\n",
        "# Entropy ‚âà 0.72.\n",
        "\n",
        "# conclusion\n",
        "#              And decision trees work by splitting data\n",
        "#               in a way that reduces uncertainty (entropy) step by step.\n",
        "\n",
        "#  2. for a class problem the min entropy is 0 and the\n",
        "#    max is 1\n",
        "\n",
        "# Entropy in classification problems:\n",
        "\n",
        "# Minimum Entropy = 0\n",
        "# ‚Üí This happens when the node is pure (all samples belong to the same class).\n",
        "# Example: 100% Dog, 0% Cat ‚Üí Entropy = 0\n",
        "\n",
        "# Maximum Entropy = log‚ÇÇ(c), where c = number of classes\n",
        "# ‚Üí For binary classification (c = 2): max entropy = log‚ÇÇ(2) = 1\n",
        "# This happens when the classes are evenly split (50% Dog, 50% Cat).\n",
        "\n",
        "# ‚Üí For 3 classes (c = 3): max entropy = log‚ÇÇ(3) ‚âà 1.585\n",
        "# This happens when all three classes are equally distributed (1/3 each).\n",
        "\n",
        "# Summary:\n",
        "\n",
        "# For binary classification ‚Üí min entropy = 0, max entropy = 1\n",
        "\n",
        "\n",
        "\n",
        "#  for multi  classifiction\n",
        "#  For multi-class classification ‚Üí\n",
        "#   min entropy = 0, max entropy = log‚ÇÇ(c)\n",
        "\n",
        "# both log2 or log e can be used to calculate entropy\n",
        "\n"
      ],
      "metadata": {
        "id": "-YrmSI8lkRQD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Entropy and KDE\n",
        "\n",
        "# If KDE is very flat (low peakness) ‚Üí data is spread out\n",
        "#  ‚Üí high entropy (more uncertainty).\n",
        "\n",
        "# If KDE is very sharp (high peakness) ‚Üí\n",
        "#  data is concentrated in a small region ‚Üí low entropy (less uncertainty).\n",
        "\n",
        "#   Think of What Entropy Measures\n",
        "\n",
        "# Entropy = average uncertainty / unpredictability.\n",
        "\n",
        "# If the probability distribution is sharp (high peakness) ‚Üí\n",
        "#  most of the probability mass is concentrated in a small region ‚Üí\n",
        "#  outcomes are more predictable ‚Üí entropy is low.\n",
        "\n",
        "# If the probability distribution is flat (low peakness) ‚Üí\n",
        "# probability mass is spread over a wide region ‚Üí\n",
        "#  outcomes can occur in many places ‚Üí more uncertainty ‚Üí entropy is high."
      ],
      "metadata": {
        "id": "GqoXHDOO48Pu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entropy\n",
        "\n",
        "#    Meaning of Entropy\n",
        "\n",
        "# Entropy is a measure of uncertainty, impurity, or randomness in the data.\n",
        "\n",
        "# When the classes are mixed , entropy is high.\n",
        "\n",
        "# When the data is pure (all samples belong to one class),\n",
        "#  entropy is low (zero).\n",
        "\n",
        "#  Parent Entropy = 1 (What it Means)\n",
        "\n",
        "# In your case, the parent node has:\n",
        "\n",
        "# Yes = 4 (50%)\n",
        "\n",
        "# No = 4 (50%)\n",
        "\n",
        "# This is a perfectly balanced distribution (50‚Äì50).\n",
        "\n",
        "#  It means we are completely uncertain about the next sample ‚Üí\n",
        "#  it could be Yes or No with equal probability.\n",
        "\n",
        "# Therefore, entropy takes its maximum value = 1 bit\n",
        "#  (for binary classification).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Comparison Cases\n",
        "\n",
        "# All Yes (8 Yes, 0 No):\n",
        "\n",
        "# H = 0\n",
        "\n",
        "# ‚Üí No uncertainty (we are fully sure outcome = Yes).\n",
        "\n",
        "# All No (0 Yes, 8 No):\n",
        "\n",
        "\n",
        "# H = 0\n",
        "\n",
        "# ‚Üí No uncertainty (we are fully sure outcome = No).\n",
        "\n",
        "# 50‚Äì50 split (4 Yes, 4 No):\n",
        "\n",
        "# H = 1\n",
        "\n",
        "# ‚Üí Maximum uncertainty (highest confusion).\n"
      ],
      "metadata": {
        "id": "KulUKS_Q3lAe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  So, Parent Entropy = 1 means the data is maximally impure\n",
        "#            (50% Yes, 50% No).\n",
        "#  It‚Äôs the highest possible uncertainty for binary classification."
      ],
      "metadata": {
        "id": "ScCmxG6u5IuU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Information Gain (IG)\n",
        "\n",
        "# Definition: Information Gain is a metric used in Decision Trees\n",
        "# to measure how much a feature reduces the uncertainty (entropy) in the data.\n",
        "\n",
        "# It basically tells:\n",
        "\n",
        "#  ‚ÄúHow good is this feature at splitting the data into pure groups?‚Äù\n",
        "\n",
        "\n",
        "\n",
        "#  In One Line\n",
        "\n",
        "# Information Gain = Reduction in uncertainty (entropy)\n",
        "# after splitting on a feature."
      ],
      "metadata": {
        "id": "xIBp_87A5aid"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Information Gain Calculation\n",
        "\n",
        "\n",
        "# We have 8 samples in the parent node:\n",
        "\n",
        "# Yes = 4\n",
        "\n",
        "# No = 4\n",
        "\n",
        "# Step 1: Parent Entropy\n",
        "# H(Parent) = -(0.5 * log2(0.5) + 0.5 * log2(0.5)) = 1\n",
        "\n",
        "# Step 2: Split on the feature \"Outlook\"\n",
        "# After splitting, the data becomes:\n",
        "\n",
        "# Sunny group (4 samples): 3 Yes, 1 No\n",
        "\n",
        "# Rainy group (4 samples): 1 Yes, 3 No\n",
        "\n",
        "# Step 3: Child Entropies\n",
        "\n",
        "\n",
        "# Sunny group (3 Yes, 1 No):\n",
        "# H(Sunny) = -(3/4 * log2(3/4) + 1/4 * log2(1/4)) = 0.811\n",
        "\n",
        "# Rainy group (1 Yes, 3 No):\n",
        "# H(Rainy) = -(1/4 * log2(1/4) + 3/4 * log2(3/4)) = 0.811\n",
        "\n",
        "# Step 4: Weighted Average Entropy of Children\n",
        "# H(Children) = (4/8 * 0.811) + (4/8 * 0.811) = 0.811\n",
        "\n",
        "# Step 5: Information Gain\n",
        "# IG = H(Parent) - H(Children)\n",
        "# IG = 1 - 0.811 = 0.189\n",
        "\n",
        "# Final Result:\n",
        "# The Information Gain for splitting on \"Outlook\" = 0.189 bits.\n",
        "\n",
        "# Interpretation:\n",
        "\n",
        "# This feature reduces uncertainty a little, but not perfectly.\n",
        "# If one branch had all Yes and the other had all No,\n",
        "# then IG would be 1 (maximum)."
      ],
      "metadata": {
        "id": "F4E4no1G6Fzh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parent Entropy = Entropy of the entire dataset before any split\n",
        "\n",
        "# Examples:\n",
        "\n",
        "# All samples same (Yes, Yes, Yes, ‚Ä¶) ‚Üí Parent Entropy = 0\n",
        "\n",
        "# Half Yes, half No ‚Üí Parent Entropy = 1 (maximum)\n",
        "\n",
        "# 6 Yes, 2 No ‚Üí Parent Entropy = 0.811 (some uncertainty)\n",
        "\n"
      ],
      "metadata": {
        "id": "uBy5tYuK9MvH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gini Impurity:\n",
        "\n",
        "# Gini Impurity measures the probability of incorrectly classifying\n",
        "#  a randomly chosen element if it was labeled\n",
        "#  according to the distribution of classes in the dataset.\n",
        "\n",
        "# Formula: Gini = 1 - Œ£ (pi¬≤)\n",
        "# (where pi is the probability of class i)\n",
        "\n",
        "# If Gini = 0 ‚Üí dataset is pure (all samples belong to one class).\n",
        "\n",
        "# Higher Gini ‚Üí more mixed classes, more impurity."
      ],
      "metadata": {
        "id": "h3UYnF_uHGqV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gini vs entropy\n",
        "\n",
        "# Difference between Gini Impurity and Entropy\n",
        "\n",
        "# Definition\n",
        "\n",
        "# Entropy measures the amount of information (or uncertainty) in the dataset.\n",
        "\n",
        "# Gini Impurity measures the probability of misclassifying\n",
        "# a randomly chosen sample.\n",
        "\n",
        "# Formula\n",
        "\n",
        "# Entropy = ‚Äì Œ£ (pi * log‚ÇÇ pi)\n",
        "\n",
        "# Gini = 1 ‚Äì Œ£ (pi¬≤)\n",
        "\n",
        "# Range\n",
        "\n",
        "# Entropy: 0 to 1 (for binary classification).\n",
        "\n",
        "# Gini: 0 to 0.5 (for binary classification).\n",
        "\n",
        "# Interpretation\n",
        "\n",
        "# Entropy is based on information theory (information gain).\n",
        "\n",
        "# Gini is based on probability of misclassification.\n",
        "\n",
        "# Speed\n",
        "\n",
        "# Entropy is slower to compute (because of log).\n",
        "\n",
        "# Gini is faster (no log).\n",
        "\n",
        "# Tree Splitting\n",
        "\n",
        "# Both often give similar splits.\n",
        "\n",
        "# But Gini tends to isolate the most frequent class,\n",
        "#  while Entropy is more sensitive to class distribution.\n"
      ],
      "metadata": {
        "id": "VrROcJIKHwtU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relation between Information Gain and Impurity\n",
        "\n",
        "\n",
        "# When we split a dataset, the impurity (Entropy or Gini) decreases.\n",
        "\n",
        "# The greater the decrease in impurity, the higher the Information Gain.\n",
        "\n",
        "# Formula:\n",
        "# Information Gain = Parent Impurity ‚Äì Weighted Average of Child Impurities\n",
        "\n",
        "# So:\n",
        "\n",
        "# Higher Information Gain ‚Üí Better split (less impurity in child nodes)\n",
        "\n",
        "# Lower Information Gain ‚Üí Poor split (impurity is still high)\n",
        "\n",
        "# Conclusion:\n",
        "# More Information Gain = Less Impurity"
      ],
      "metadata": {
        "id": "OHIKODdZN-bf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "L9A2dkdhF3mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Decision tree how it works"
      ],
      "metadata": {
        "id": "1nM1ZZ1lFyI5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  How a Decision Tree works\n",
        "\n",
        "# A Decision Tree keeps splitting data into left and right\n",
        "#  branches until it reaches small groups (leaf nodes).\n",
        "\n",
        "# At the root, you have all rows (200 in your example).\n",
        "\n",
        "# After each split, the rows are divided into left and right branches.\n",
        "\n",
        "# At each node, the tree decides whether to stop and make a decision\n",
        "# or to keep splitting further.\n",
        "\n",
        "# Your sir‚Äôs example\n",
        "\n",
        "# Root Node (200 rows)\n",
        "\n",
        "# First split ‚Üí Left = 140 rows, Right = 60 rows\n",
        "\n",
        "# Left Node (140 rows)\n",
        "\n",
        "# Next split ‚Üí Left = 100 rows, Right = 40 rows\n",
        "\n",
        "# Left of 100 Node (100 rows)\n",
        "\n",
        "# Next split ‚Üí Left = 2 rows, Right = 98 rows\n",
        "\n",
        "# What ‚ÄúDecision depends here‚Äù means\n",
        "\n",
        "# The final prediction always depends on the leaf node\n",
        "# where the sample ends up.\n",
        "\n",
        "# If a leaf node has only 2 rows and both belong to the same class ‚Üí\n",
        "# the decision is fixed for that path.\n",
        "\n",
        "# So if a new sample satisfies all conditions to reach that leaf,\n",
        "# the tree will predict the class based on those 2 rows only.\n",
        "\n",
        "# That‚Äôs why your sir said: ‚ÄúDecision depends here.‚Äù ‚Üí\n",
        "# It means the final prediction depends on the distribution of data\n",
        "# inside that small node.\n",
        "\n",
        "# Example to make it clearer\n",
        "\n",
        "# Suppose we‚Äôre predicting ‚ÄúBuy Product (Yes/No)‚Äù\n",
        "\n",
        "# Root (200 people)\n",
        "\n",
        "# Split: Age < 40 ‚Üí Left = 140, Right = 60\n",
        "\n",
        "# Left branch (140): Split Salary < 50k ‚Üí Left = 100, Right = 40\n",
        "\n",
        "# Left of 100: Split Gender = Female ‚Üí Left = 2 (both ‚ÄúYes‚Äù), Right = 98 (mixed)\n",
        "\n",
        "# If a new sample is Female + Salary < 50k + Age < 40 ‚Üí\n",
        "# It will fall into the 2-row leaf, where both are ‚ÄúYes‚Äù.\n",
        "# So the Decision Tree will always predict Yes for this case."
      ],
      "metadata": {
        "id": "y3c4VK7BGnhs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**page no 7 case discussing we are here**"
      ],
      "metadata": {
        "id": "vFJB7hZllHxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Explanation\n",
        "\n",
        "# At the root node, the split is based on Height ‚â§ 174.9.\n",
        "\n",
        "# If Height ‚â§ 174.9 ‚Üí go to the left branch.\n",
        "\n",
        "# Then check Weight ‚â§ 66.5:\n",
        "\n",
        "# If Weight ‚â§ 66.5 ‚Üí Prediction = No\n",
        "\n",
        "# If Weight > 66.5 ‚Üí Prediction = Yes\n",
        "\n",
        "# If Height > 174.9 (i.e., 175‚Äì180 and beyond) ‚Üí go to the right branch.\n",
        "\n",
        "# Then check Weight ‚â§ 76.5:\n",
        "\n",
        "# If Weight ‚â§ 76.5 ‚Üí Prediction = Yes\n",
        "\n",
        "# If Weight > 76.5 ‚Üí Prediction = Yes\n",
        "\n",
        "# So basically, in the right branch (Height > 174.9),\n",
        "#  regardless of weight, the result is always Yes.\n",
        "\n",
        "# Overfitting Case\n",
        "\n",
        "# If the dataset had very complex splits\n",
        "#  (e.g., separate conditions for Height = 175, 176, 177, ‚Ä¶, 180,\n",
        "#   or very fine-grained weight intervals like 66.5, 67.2, 67.8, etc.),\n",
        "#   the tree would become unnecessarily large\n",
        "#   and memorize the training data instead of learning general rules.\n",
        "\n",
        "# This is called overfitting.\n",
        "\n",
        "# A simple tree like the one above is not overfitted,\n",
        "#  because the right side (Height > 174.9)\n",
        "#   makes the same prediction (Yes) for all weights\n",
        "# instead of splitting further."
      ],
      "metadata": {
        "id": "mchOsVOalGP_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#        example of page no 1 nitin sir refine example\n",
        "\n",
        "#               (200)\n",
        "#              /     \\\n",
        "#          (140)     (60)\n",
        "#          /   \\\n",
        "#      (100)   (40)\n",
        "#         \\\n",
        "#         (2)\n",
        "\n",
        "# Example Tree (the one you gave):\n",
        "\n",
        "\n",
        "# Explanation of Overfitting in This Case:\n",
        "\n",
        "# At the root (200), the split into 140 vs 60 seems okay\n",
        "# because both groups are reasonably large.\n",
        "\n",
        "# At 140 ‚Üí split into 100 vs 40, this is also fine\n",
        "#  because both are still meaningful groups.\n",
        "\n",
        "# But then at 100 ‚Üí split into 98 vs 2 (for example, your \"2\"),\n",
        "# this is where the problem comes:\n",
        "\n",
        "# The tree is trying too hard to separate even the tiny number\n",
        "# of samples (just 2).\n",
        "\n",
        "# This small branch does not generalize well;\n",
        "# it is only capturing noise or outliers in training data.\n",
        "\n",
        "# In testing (new/unseen data),\n",
        "# such tiny splits rarely help and usually hurt performance.\n",
        "\n",
        "# Why is this Overfitting?\n",
        "\n",
        "# The model is fitting the training data perfectly,\n",
        "#         even for very small groups.\n",
        "\n",
        "# Instead of learning the general pattern\n",
        "#  (big splits like 200‚Üí140/60 or 140‚Üí100/40),\n",
        "#   it goes deep to create rules for just 2 samples.\n",
        "\n",
        "# These small branches are not representative of the real distribution.\n",
        "\n",
        "#  So, overfitting happens when your tree keeps splitting\n",
        "# until very small nodes (like 2 samples) are created.\n",
        "#  A better approach is to prune the tree or set a minimum samples\n",
        "#  per leaf (e.g., at least 10 samples before a split).\n"
      ],
      "metadata": {
        "id": "YfshsxmKpwVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if Study Hours ‚â§ 2 and Pen = Red ‚Üí Fail\n",
        "\n",
        "# if Study Hours ‚â§ 2 and Pen = Blue ‚Üí Pass\n",
        "\n",
        "\n",
        "# means  if ant test data coming  coming for less than 2 hr type\n",
        "# then it may mistake more because  it have not understands\n",
        "# pattern just learnt  yes and no by two condition\n",
        "\n",
        "# hence it is the case of overfitting\n",
        "\n"
      ],
      "metadata": {
        "id": "KWhba1HNvQzl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_PISxAeI8xqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}