{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxqJpbGpE3S5vl/Sx6osOE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88lL5iE2gPqS"
      },
      "outputs": [],
      "source": [
        "#  What is Entropy in Decision Trees?\n",
        "\n",
        "# Entropy measures the impurity (uncertainty) in a dataset.\n",
        "\n",
        "# A decision tree uses entropy to decide where to split the data.\n",
        "\n",
        "# Goal: split the data in such a way that each branch\n",
        "# becomes more pure (less entropy).\n",
        "\n",
        "#   purity vs impurity\n",
        "\n",
        "# üîπ Purity\n",
        "\n",
        "# A node (or dataset) is called pure if all the samples\n",
        "# inside it belong to the same class.\n",
        "\n",
        "# Example:\n",
        "\n",
        "# If a node contains 100 animals and all are Dogs, then the node is pure.\n",
        "\n",
        "# In this case, there is no uncertainty.\n",
        "\n",
        "# Entropy = 0, Gini = 0.\n",
        "\n",
        "# Pure means perfectly classified (no mix of classes).\n",
        "\n",
        "#  Impurity\n",
        "\n",
        "# A node is impure if it contains a mix of different classes.\n",
        "\n",
        "# Example:\n",
        "\n",
        "# If a node has 50 Dogs and 50 Cats ‚Üí maximum impurity (most uncertain).\n",
        "\n",
        "# If a node has 80 Dogs and 20 Cats ‚Üí still impure,\n",
        "#  but less than the 50‚Äì50 case.\n",
        "\n",
        "#  Impure means uncertain / mixed classes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How Decision Trees Use This\n",
        "\n",
        "\n",
        "# Decision trees split data so that impurity decreases after each split.\n",
        "\n",
        "# Measures of impurity:\n",
        "\n",
        "# Entropy ‚Üí 0 (pure) to 1 (most impure in binary case).\n",
        "\n",
        "# Gini Index ‚Üí 0 (pure) to 0.5 (most impure in binary case)."
      ],
      "metadata": {
        "id": "R3Nbjn6Pgk-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gini\n",
        "\n",
        "# üîπ Properties\n",
        "\n",
        "# Pure node ‚Üí only one class present ‚Üí Gini = 0\n",
        "\n",
        "# Most impure node (equal distribution of classes) ‚Üí Gini is maximum"
      ],
      "metadata": {
        "id": "6u1_b2YUixIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# observation\n",
        "\n",
        "#  1. more than the uncertainity more is entropy\n",
        "\n",
        "# define\n",
        "\n",
        "#  Entropy and Uncertainty\n",
        "\n",
        "# Entropy is a measure of uncertainty (or impurity).\n",
        "\n",
        "# If the outcome is certain ‚Üí entropy is low (0).\n",
        "\n",
        "# If the outcome is uncertain / random ‚Üí entropy is high.\n",
        "\n",
        "\n",
        "# example\n",
        "\n",
        "#  Binary Classification Example\n",
        "\n",
        "# 100% Dog (no Cat)\n",
        "\n",
        "# No uncertainty, you already know the class.\n",
        "\n",
        "# Entropy = 0 (pure).\n",
        "\n",
        "# 50% Dog, 50% Cat\n",
        "\n",
        "# Maximum uncertainty ‚Üí you can‚Äôt predict better than random guess.\n",
        "\n",
        "# Entropy = 1 (maximum).\n",
        "\n",
        "# 80% Dog, 20% Cat\n",
        "\n",
        "# Some uncertainty (but less than 50‚Äì50).\n",
        "\n",
        "# Entropy ‚âà 0.72.\n",
        "\n",
        "# conclusion\n",
        "#              And decision trees work by splitting data\n",
        "#               in a way that reduces uncertainty (entropy) step by step.\n",
        "\n",
        "#  2. for a class problem the min entropy is 0 and the\n",
        "#    max is 1\n",
        "\n",
        "# Entropy in classification problems:\n",
        "\n",
        "# Minimum Entropy = 0\n",
        "# ‚Üí This happens when the node is pure (all samples belong to the same class).\n",
        "# Example: 100% Dog, 0% Cat ‚Üí Entropy = 0\n",
        "\n",
        "# Maximum Entropy = log‚ÇÇ(c), where c = number of classes\n",
        "# ‚Üí For binary classification (c = 2): max entropy = log‚ÇÇ(2) = 1\n",
        "# This happens when the classes are evenly split (50% Dog, 50% Cat).\n",
        "\n",
        "# ‚Üí For 3 classes (c = 3): max entropy = log‚ÇÇ(3) ‚âà 1.585\n",
        "# This happens when all three classes are equally distributed (1/3 each).\n",
        "\n",
        "# Summary:\n",
        "\n",
        "# For binary classification ‚Üí min entropy = 0, max entropy = 1\n",
        "\n",
        "\n",
        "\n",
        "#  for multi  classifiction\n",
        "#  For multi-class classification ‚Üí\n",
        "#   min entropy = 0, max entropy = log‚ÇÇ(c)\n",
        "\n",
        "# both log2 or log e can be used to calculate entropy\n",
        "\n"
      ],
      "metadata": {
        "id": "-YrmSI8lkRQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Entropy and KDE\n",
        "\n",
        "# If KDE is very flat (low peakness) ‚Üí data is spread out\n",
        "#  ‚Üí high entropy (more uncertainty).\n",
        "\n",
        "# If KDE is very sharp (high peakness) ‚Üí\n",
        "#  data is concentrated in a small region ‚Üí low entropy (less uncertainty).\n",
        "\n",
        "#   Think of What Entropy Measures\n",
        "\n",
        "# Entropy = average uncertainty / unpredictability.\n",
        "\n",
        "# If the probability distribution is sharp (high peakness) ‚Üí\n",
        "#  most of the probability mass is concentrated in a small region ‚Üí\n",
        "#  outcomes are more predictable ‚Üí entropy is low.\n",
        "\n",
        "# If the probability distribution is flat (low peakness) ‚Üí\n",
        "# probability mass is spread over a wide region ‚Üí\n",
        "#  outcomes can occur in many places ‚Üí more uncertainty ‚Üí entropy is high."
      ],
      "metadata": {
        "id": "GqoXHDOO48Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entropy\n",
        "\n",
        "#        Meaning of Entropy\n",
        "\n",
        "# Entropy is a measure of uncertainty, impurity, or randomness in the data.\n",
        "\n",
        "# When the classes are mixed, entropy is high.\n",
        "\n",
        "# When the data is pure (all samples belong to one class),\n",
        "#  entropy is low (zero).\n",
        "\n",
        "#  Parent Entropy = 1 (What it Means)\n",
        "\n",
        "# In your case, the parent node has:\n",
        "\n",
        "# Yes = 4 (50%)\n",
        "\n",
        "# No = 4 (50%)\n",
        "\n",
        "# This is a perfectly balanced distribution (50‚Äì50).\n",
        "\n",
        "#  It means we are completely uncertain about the next sample ‚Üí\n",
        "#  it could be Yes or No with equal probability.\n",
        "\n",
        "# Therefore, entropy takes its maximum value = 1 bit\n",
        "#  (for binary classification).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Comparison Cases\n",
        "\n",
        "# All Yes (8 Yes, 0 No):\n",
        "\n",
        "# H = 0\n",
        "\n",
        "# ‚Üí No uncertainty (we are fully sure outcome = Yes).\n",
        "\n",
        "# All No (0 Yes, 8 No):\n",
        "\n",
        "\n",
        "# H = 0\n",
        "\n",
        "# ‚Üí No uncertainty (we are fully sure outcome = No).\n",
        "\n",
        "# 50‚Äì50 split (4 Yes, 4 No):\n",
        "\n",
        "# H = 1\n",
        "\n",
        "# ‚Üí Maximum uncertainty (highest confusion).\n"
      ],
      "metadata": {
        "id": "KulUKS_Q3lAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  So, Parent Entropy = 1 means the data is maximally impure\n",
        "#            (50% Yes, 50% No).\n",
        "#  It‚Äôs the highest possible uncertainty for binary classification."
      ],
      "metadata": {
        "id": "ScCmxG6u5IuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Information Gain (IG)\n",
        "\n",
        "# Definition: Information Gain is a metric used in Decision Trees\n",
        "# to measure how much a feature reduces the uncertainty (entropy) in the data.\n",
        "\n",
        "# It basically tells:\n",
        "\n",
        "#  ‚ÄúHow good is this feature at splitting the data into pure groups?‚Äù\n",
        "\n",
        "\n",
        "\n",
        "#  In One Line\n",
        "\n",
        "# Information Gain = Reduction in uncertainty (entropy)\n",
        "# after splitting on a feature."
      ],
      "metadata": {
        "id": "xIBp_87A5aid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Information Gain Calculation\n",
        "\n",
        "\n",
        "# We have 8 samples in the parent node:\n",
        "\n",
        "# Yes = 4\n",
        "\n",
        "# No = 4\n",
        "\n",
        "# Step 1: Parent Entropy\n",
        "# H(Parent) = -(0.5 * log2(0.5) + 0.5 * log2(0.5)) = 1\n",
        "\n",
        "# Step 2: Split on the feature \"Outlook\"\n",
        "# After splitting, the data becomes:\n",
        "\n",
        "# Sunny group (4 samples): 3 Yes, 1 No\n",
        "\n",
        "# Rainy group (4 samples): 1 Yes, 3 No\n",
        "\n",
        "# Step 3: Child Entropies\n",
        "\n",
        "\n",
        "# Sunny group (3 Yes, 1 No):\n",
        "# H(Sunny) = -(3/4 * log2(3/4) + 1/4 * log2(1/4)) = 0.811\n",
        "\n",
        "# Rainy group (1 Yes, 3 No):\n",
        "# H(Rainy) = -(1/4 * log2(1/4) + 3/4 * log2(3/4)) = 0.811\n",
        "\n",
        "# Step 4: Weighted Average Entropy of Children\n",
        "# H(Children) = (4/8 * 0.811) + (4/8 * 0.811) = 0.811\n",
        "\n",
        "# Step 5: Information Gain\n",
        "# IG = H(Parent) - H(Children)\n",
        "# IG = 1 - 0.811 = 0.189\n",
        "\n",
        "# Final Result:\n",
        "# The Information Gain for splitting on \"Outlook\" = 0.189 bits.\n",
        "\n",
        "# Interpretation:\n",
        "\n",
        "# This feature reduces uncertainty a little, but not perfectly.\n",
        "# If one branch had all Yes and the other had all No,\n",
        "# then IG would be 1 (maximum)."
      ],
      "metadata": {
        "id": "F4E4no1G6Fzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Parent Entropy = Entropy of the entire dataset before any split\n",
        "\n",
        "Examples:\n",
        "\n",
        "All samples same (Yes, Yes, Yes, ‚Ä¶) ‚Üí Parent Entropy = 0\n",
        "\n",
        "Half Yes, half No ‚Üí Parent Entropy = 1 (maximum)\n",
        "\n",
        "6 Yes, 2 No ‚Üí Parent Entropy = 0.811 (some uncertainty)\n",
        "\n"
      ],
      "metadata": {
        "id": "uBy5tYuK9MvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gini Impurity:\n",
        "\n",
        "# Gini Impurity measures the probability of incorrectly classifying\n",
        "#  a randomly chosen element if it was labeled\n",
        "#  according to the distribution of classes in the dataset.\n",
        "\n",
        "# Formula: Gini = 1 - Œ£ (pi¬≤)\n",
        "# (where pi is the probability of class i)\n",
        "\n",
        "# If Gini = 0 ‚Üí dataset is pure (all samples belong to one class).\n",
        "\n",
        "# Higher Gini ‚Üí more mixed classes, more impurity."
      ],
      "metadata": {
        "id": "h3UYnF_uHGqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gini vs entropy\n",
        "\n",
        "# Difference between Gini Impurity and Entropy\n",
        "\n",
        "# Definition\n",
        "\n",
        "# Entropy measures the amount of information (or uncertainty) in the dataset.\n",
        "\n",
        "# Gini Impurity measures the probability of misclassifying\n",
        "# a randomly chosen sample.\n",
        "\n",
        "# Formula\n",
        "\n",
        "# Entropy = ‚Äì Œ£ (pi * log‚ÇÇ pi)\n",
        "\n",
        "# Gini = 1 ‚Äì Œ£ (pi¬≤)\n",
        "\n",
        "# Range\n",
        "\n",
        "# Entropy: 0 to 1 (for binary classification).\n",
        "\n",
        "# Gini: 0 to 0.5 (for binary classification).\n",
        "\n",
        "# Interpretation\n",
        "\n",
        "# Entropy is based on information theory (information gain).\n",
        "\n",
        "# Gini is based on probability of misclassification.\n",
        "\n",
        "# Speed\n",
        "\n",
        "# Entropy is slower to compute (because of log).\n",
        "\n",
        "# Gini is faster (no log).\n",
        "\n",
        "# Tree Splitting\n",
        "\n",
        "# Both often give similar splits.\n",
        "\n",
        "# But Gini tends to isolate the most frequent class,\n",
        "#  while Entropy is more sensitive to class distribution.\n"
      ],
      "metadata": {
        "id": "VrROcJIKHwtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relation between Information Gain and Impurity\n",
        "\n",
        "\n",
        "# When we split a dataset, the impurity (Entropy or Gini) decreases.\n",
        "\n",
        "# The greater the decrease in impurity, the higher the Information Gain.\n",
        "\n",
        "# Formula:\n",
        "# Information Gain = Parent Impurity ‚Äì Weighted Average of Child Impurities\n",
        "\n",
        "# So:\n",
        "\n",
        "# Higher Information Gain ‚Üí Better split (less impurity in child nodes)\n",
        "\n",
        "# Lower Information Gain ‚Üí Poor split (impurity is still high)\n",
        "\n",
        "# Conclusion:\n",
        "# More Information Gain = Less Impurity"
      ],
      "metadata": {
        "id": "OHIKODdZN-bf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}